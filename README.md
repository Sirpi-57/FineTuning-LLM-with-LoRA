# FineTuning-LLM-with-LoRA
This project leverages a LoRA (Low-Rank Adaptation) model for fine-tuning a pre-trained large language model (Qwen2.5-1.5B) to respond to task-specific instructions. The dataset used for training comes from the LaMini-instruction dataset, which contains a variety of task instructions paired with appropriate responses.
